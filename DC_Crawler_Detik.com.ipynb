{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31729882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "search_terms = \"krisis iklim\"  # Modify this string to use different search terms\n",
    "site_id = 2  # Adjust site ID if necessary\n",
    "sort_by = 'time'  # Can be changed to other sorting parameters like relevance, etc.\n",
    "max_pages_per_run = 100  # Maximum pages to scrape per run\n",
    "total_pages = 161  # Total number of pages to scrape\n",
    "start_page = 100  # Starting page\n",
    "\n",
    "# Create URL and headers\n",
    "base_url = f'https://www.detik.com/search/searchall?query={search_terms}&siteid={site_id}&sortby={sort_by}&page='\n",
    "headers = {\n",
    "    \"User-Agent\": \"ResearchScraper (email): This bot is conducting research on climate change coverage in Indonesia for non-profit academic purposes.\"\n",
    "}\n",
    "\n",
    "# Prepare data collection\n",
    "data = []\n",
    "pages_scraped = 0\n",
    "empty_pages_in_a_row = 0\n",
    "\n",
    "# Scraping loop\n",
    "for i in range(start_page, start_class + min(max_pages_per_run, total_pages)):\n",
    "    if empty_pages_in_a_row >= 3:  # Stop if 3 consecutive empty pages are encountered\n",
    "        print(\"No more articles found. Stopping.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Scraping page {i}...\")\n",
    "    url = base_url + str(i)\n",
    "\n",
    "    try:\n",
    "        # Fetch the page\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract articles\n",
    "        articles = soup.find_all('article')\n",
    "        if not articles:\n",
    "            print(f\"No articles on page {i}.\")\n",
    "            empty_pages_in_a_row += 1\n",
    "            continue\n",
    "\n",
    "        empty_pages_in_a_row = 0  # Reset the counter if the page has articles\n",
    "        pages_scraped += 1\n",
    "\n",
    "        # Process each article\n",
    "        for article in articles:\n",
    "            title = article.find('h2', class_='title').get_text(strip=True)\n",
    "            link = article.find('a')['href']\n",
    "            description = article.find('p').get_text(strip=True)\n",
    "            date = article.find('span', class_='date').get_text(strip=True)\n",
    "\n",
    "            # Fetch the full article content\n",
    "            article_response = requests.get(link, headers=headers)\n",
    "            article_response.raise_for_status()\n",
    "            article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            article_div = article_soup.find('div', class_='detail__body-text itp_bodycontent')\n",
    "            article_text = article_div.get_text(strip=True) if article_div else \"N/A\"\n",
    "\n",
    "            # Save the extracted data\n",
    "            data.append([title, link, description, date, article_text])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping page {i}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "column_names = ['Title', 'Link', 'Description', 'Date', 'Article']\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "print(df)\n",
    "\n",
    "# Save data to CSV\n",
    "filename = f\"Data_{search_terms.replace(' ', '_')}_{start_page}_{start_page+pages_scraped-1}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
