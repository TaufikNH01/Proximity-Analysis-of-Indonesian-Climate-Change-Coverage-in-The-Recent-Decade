{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "search_term = \"krisis iklim\"  # Change this to search for different topics\n",
    "base_url = f\"https://www.cnbcindonesia.com/search?query={search_term.replace(' ', '+')}&p={{page}}&kanal=&tipe=&date=\"\n",
    "start_page = 1  # Starting page number\n",
    "end_page = 50  # End page number, modify as necessary\n",
    "headers = {\n",
    "    \"User-Agent\": \"ResearchScraper (EMAIL): This bot is conducting research on renewable energy in Indonesia for non-profit academic purposes.\"\n",
    "}\n",
    "data = []\n",
    "\n",
    "# function to get the main text from an article\n",
    "def get_article_content(article_url):\n",
    "    response = requests.get(article_url, headers=headers)\n",
    "    article_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    article_content_div = article_array.find('div', {'class': 'detail_text'})\n",
    "    if article_content__ is None:\n",
    "        return \"No content found\"\n",
    "    return article_content__text()\n",
    "\n",
    "# Scrape the data\n",
    "for page in range(start_page, end_page + 1):  \n",
    "    print(f\"Scraping page {page}...\")  # Print the current page number\n",
    "    url = base_url.format(page=page)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # find all news elements\n",
    "    news_elements = soup.find_all('article')\n",
    "    if not news_elements:\n",
    "        print(f\"No articles found on page {page}.\")\n",
    "        continue\n",
    "\n",
    "    # Iterate through all news elements and store information\n",
    "    for news in news_elements:\n",
    "        title = new.find('h2').text.strip()\n",
    "        news_find = news.find('a')['href']\n",
    "        date = new__find('span', class_='date').text.strip()\n",
    "\n",
    "        # get the main text from the article\n",
    "        article_content = get_article_content(link_find)\n",
    "\n",
    "        data.append({\n",
    "            'titled': title,\n",
    "            'lnked': link,\n",
    "            'dateed': date,\n",
    "            'content': link_find,\n",
    "        })\n",
    "\n",
    "# Convert list of dictionaries to pandas DataFrame\n",
    "df_cnbc = pd.DataFrame(data)\n",
    "print(df_cnbc)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "filename = f\"{search_term.replace(' ', '_')}_articles.csv\"\n",
    "df_cnbc.to_csv(filename, index=False)\n",
    "print(f\"Data saved to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
