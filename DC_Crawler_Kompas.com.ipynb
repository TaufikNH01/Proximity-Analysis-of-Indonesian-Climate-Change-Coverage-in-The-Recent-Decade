{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236367d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "search_terms = \"perubahan iklim\"  # Modify this string to use different search terms\n",
    "base_url = f'https://www.kompas.com/tag/{search_terms.replace(\" \", \"+\")}'\n",
    "headers = {\n",
    "    'User-Agent': 'ResearchScraper (taufik.impact@gmail.com): This bot is conducting research on climate change in Indonesia for non-profit academic purposes.'\n",
    "}\n",
    "start_page = 1  # Starting page\n",
    "total_pages = 161  # Total pages available to scrape\n",
    "max_pages_per_run = 50  # Maximum number of pages to scrape in one run\n",
    "\n",
    "# Function to scrape the content of an article\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Ensure the response is successful\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_content_div = soup.find('div', class_='read__content')\n",
    "        if article_content_div:\n",
    "            article_paragraphs = article_content_div.find_all('p')\n",
    "            article_text = ' '.join(paragraph.get_text(strip=True) for paragraph in article_paragraphs)\n",
    "            return article_text\n",
    "        else:\n",
    "            return \"Article content not found\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(page_url):\n",
    "    news_data = []\n",
    "    try:\n",
    "        response = requests.get(page_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        news_items = soup.find_all('div', class_='article__list')\n",
    "\n",
    "        if not news_items:\n",
    "            print(\"No news articles found on the page.\")\n",
    "            return news_data  # Returns an empty list if no articles are found\n",
    "\n",
    "        for item in news_items:\n",
    "            try:\n",
    "                title_element = item.find('a', class_='article__link')\n",
    "                date_element = item.find('div', class_='article__date')\n",
    "                segment_element = item.find('div', class_='article__subtitle')\n",
    "\n",
    "                title = title_element.text.strip() if title_element else 'Title not found'\n",
    "                link = title_element['href'] if title_element else 'Link not found'\n",
    "                date = date_element.text.strip() if date_element else 'Date not found'\n",
    "                segment = segment_element.text.strip() if segment_element else 'N/A'\n",
    "\n",
    "                article_content = scrape_article_content(link)\n",
    "                news_data.append({'title': title, 'link': link, 'date': date, 'segment': segment, 'content': article_content})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing news item: {e}\")\n",
    "                news_data.append({'title': np.nan, 'link': np.nan, 'date': np.nan, 'segment': np.nan, 'content': np.nan})\n",
    "            \n",
    "            time.sleep(1)  # Throttle requests to be polite to the server\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page: {e}\")\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# Function to scrape multiple pages within a range\n",
    "def scrape_pages_in_range(start_page, max_pages_per_run):\n",
    "    all_news_data = []\n",
    "    for page in range(start_page, min(start_page + max_pages_per_run, total_pages + 1)):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_url = f\"{base_url}?page={page}\"\n",
    "        page_data = scrape_page(page_url)\n",
    "        if not page_data:  # If a page returns no data, assume subsequent pages might be empty and stop\n",
    "            print(\"Stopping early due to consecutive empty pages.\")\n",
    "            break\n",
    "        all_news_data.extend(page_data)\n",
    "    return all_news_data\n",
    "\n",
    "# Execute the scraping\n",
    "news_data = scrape_pages_in_range(start_page, max_pages_per_run)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df_kompas = pd.DataFrame(news_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "filename = f\"kompas_{search_terms.replace(' ', '_')}_{start_page}_{start_page + len(news_data) // len(df_pi.columns) - 1}.csv\"\n",
    "df_kompas.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Scraped {len(df_kompas)} articles from pages {start_page} to {start_page + len(news_data) // len(df_pi.columns) - 1}.\")\n",
    "print(df_kompas.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
