{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d715bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs from BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "keywords = [\"pemanasan global\", \"global warming\", \"climate change\", \"climate crisis\", \"krisis iklim\"]\n",
    "base_url = \"https://www.jawapos.com/search?q=\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"ResearchScraper (EMAIL): This bot is conducting research on climate change in Indonesia for non-profit academic purposes.\"\n",
    "}\n",
    "\n",
    "# Initialize the dictionary to store DataFrames for each keyword\n",
    "dataframes = {}\n",
    "max_pages_per_keyword = 50  # Set the maximum number of pages to scrape per keyword\n",
    "\n",
    "# Loop over each keyword to perform search and fetch articles\n",
    "for keyword in keywords:\n",
    "    articles = []  # Initialize an empty list to store articles for this keyword\n",
    "    print(f\"Starting search for keyword: '{keyword}'\")\n",
    "    page_empty = False  # Flag to check if the current page is empty\n",
    "\n",
    "    for page in range(1, max_pages_per_keyword + 1):\n",
    "        if page_empty:  # Stop if the previous page had no articles\n",
    "            print(f\"No more articles found for '{keyword}' beyond page {page-1}.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Fetching page {page} for keyword: '{keyword}'\")\n",
    "        query_url = f\"{base_url}{requests.utils.quote(keyword)}&page={page}\"\n",
    "        response = requests.get(query_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        items = soup.find_all(\"div\", class_=\"latest__item\")\n",
    "\n",
    "        # Check if the page is empty\n",
    "        if not items:\n",
    "            page_empty = True\n",
    "            continue\n",
    "\n",
    "        # Extract details for each article found\n",
    "        for item in items:\n",
    "            title = item.find(\"h2\", class_=\"latest__title\").text.strip()\n",
    "            link = item.find(\"a\", class_=\"latest__link\")[\"href\"]\n",
    "            date = item.find(\"div\", class_=\"latest__date\").text.strip()  # Corrected class for date\n",
    "            articles.append({\"title\": title, \"link\": link, \"date\": date})\n",
    "\n",
    "    # Fetch the article content for each link collected\n",
    "    for article in articles:\n",
    "        try:\n",
    "            response = requests.get(article['link'], headers=headers)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            article_text = ' '.join([p.text for p in soup.find(\"article\", class_=\"read__content\").find_all(\"p\")])\n",
    "            article['content'] = article_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching article content for '{article['title']}': {e}\")\n",
    "            article['content'] = \"\"\n",
    "\n",
    "    # Convert the list of article details into a DataFrame\n",
    "    df_keyword = pd.DataFrame(articles)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file, using the keyword to name the file\n",
    "    keyword_filename = f\"{keyword.replace(' ', '_').lower()}_articles_{page-1}.csv\"  # Files named by the last scraped page\n",
    "    df_keyword.to_csv(keyword_filename, index=False)\n",
    "    print(f\"Articles for '{keyword}' saved to '{keyword_filename}'.\")\n",
    "\n",
    "    # Add the DataFrame to the `dataframes` dictionary\n",
    "    dataframes[keyword] = df_keyword\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
